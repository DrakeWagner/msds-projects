{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzEF4RodCFEY"
      },
      "source": [
        "This project works with data from the Wisconsin Breast Cancer dataset. Below, I train a logistic regression model to predict the diagnosis. Three features are used in the model. Before training the model, I apply scaling to these features using StandardScaler. I then train the model and compute the accuracy on the test set, while also computing and presenting the confusion matrix. Results are shown at the end."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-D2aaaoDCGr",
        "outputId": "104d3141-d646-40c9-f0f1-6668ffa66f85"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 32 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 53.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=45d1ba6daae209af4e2af3bd0c23d8469f50b382e7591856d9665507a5919f81\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4Fbt8R1KCFEe"
      },
      "outputs": [],
      "source": [
        "# load modules\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
        "from pyspark.mllib.regression import LabeledPoint\n",
        "from pyspark.ml.feature import VectorAssembler \n",
        "from pyspark.mllib.linalg import Vectors\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AQ2u1renCFEh"
      },
      "outputs": [],
      "source": [
        "# param init\n",
        "infile = 'wisc_breast_cancer_w_fields.csv'\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Wisc BRCA\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PRgIBwdnCFEj"
      },
      "outputs": [],
      "source": [
        "# read data into dataframe\n",
        "df = spark.read.csv(infile, inferSchema=True, header = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K97I_Wm1CFEk",
        "outputId": "08a08e5c-8546-44ab-9beb-08e92ab386e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "569"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYC1GHTaCFEn",
        "outputId": "490cdc1f-154a-4628-8045-c69552bb2ecb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id, diagnosis, f1, f2, f3, f4, f5, f6, f7, f8, f9, f10, f11, f12, f13, f14, f15, f16, f17, f18, f19, f20, f21, f22, f23, f24, f25, f26, f27, f28, f29, f30, "
          ]
        }
      ],
      "source": [
        "colnames = []\n",
        "for i in df.schema.names:\n",
        "    print(i,sep='',end=', ')\n",
        "    colnames.append(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmQ8-luiCFEo",
        "outputId": "1b674dba-936f-4e5d-9632-a33ed380541a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('id', 'int'), ('diagnosis', 'string')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# check types:\n",
        "[x for x in df.dtypes if x[1] != 'double']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFb9KM53CFEp",
        "outputId": "ff686a22-d268-4b8f-fba0-d81bc45ff7bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[f1: double, f2: double, f3: double, f4: double, f5: double, f6: double, f7: double, f8: double, f9: double, f10: double, f11: double, f12: double, f13: double, f14: double, f15: double, f16: double, f17: double, f18: double, f19: double, f20: double, f21: double, f22: double, f23: double, f24: double, f25: double, f26: double, f27: double, f28: double, f29: double, f30: double]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "df.select([i for i in df.schema.names if i not in {'id', 'diagnosis'}])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN1HJlBWCFEq"
      },
      "source": [
        "# MODEL TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YR5F3FRTCFEq"
      },
      "outputs": [],
      "source": [
        "assembler = VectorAssembler(inputCols=[\"f1\",\"f2\",\"f3\",\"f4\",\"f5\",\"f6\",\"f7\",\"f8\",\"f9\",\n",
        "                                        \"f10\",\"f11\",\"f12\",\"f13\",\"f14\",\"f15\",\"f16\",\"f17\",\n",
        "                                        \"f18\",\"f19\",\"f20\",\"f21\",\"f22\", \"f23\", \"f24\", \"f25\", \n",
        "                                       \"f26\", \"f27\", \"f28\", \"f29\", \"f30\"], \n",
        "                            outputCol=\"features\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YOCJfcAFCFEr"
      },
      "outputs": [],
      "source": [
        "# scaling\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "\n",
        "transformed = assembler.transform(df)\n",
        "scaler = StandardScaler(inputCol='features',\n",
        "                       outputCol='scaledFeatures')\n",
        "sm = scaler.fit(transformed) # model\n",
        "sd = sm.transform(transformed) # data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "H0ytwdMiCFEs"
      },
      "outputs": [],
      "source": [
        "# convert to RDD\n",
        "dataRdd = transformed.select(\"diagnosis\", \"features\").rdd.map(tuple)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TA9ctAOkCFEt",
        "outputId": "b7e3d322-29b0-4d8c-9907-0e15f84d5192"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('M',\n",
              "  DenseVector([17.99, 10.38, 122.8, 1001.0, 0.1184, 0.2776, 0.3001, 0.1471, 0.2419, 0.0787, 1.095, 0.9053, 8.589, 153.4, 0.0064, 0.049, 0.0537, 0.0159, 0.03, 0.0062, 25.38, 17.33, 184.6, 2019.0, 0.1622, 0.6656, 0.7119, 0.2654, 0.4601, 0.1189])),\n",
              " ('M',\n",
              "  DenseVector([20.57, 17.77, 132.9, 1326.0, 0.0847, 0.0786, 0.0869, 0.0702, 0.1812, 0.0567, 0.5435, 0.7339, 3.398, 74.08, 0.0052, 0.0131, 0.0186, 0.0134, 0.0139, 0.0035, 24.99, 23.41, 158.8, 1956.0, 0.1238, 0.1866, 0.2416, 0.186, 0.275, 0.089]))]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# look at some data\n",
        "dataRdd.take(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "G6s04vXnCFEu"
      },
      "outputs": [],
      "source": [
        "# map label to binary values, then convert to LabeledPoint\n",
        "lp = dataRdd.map(lambda row:(1 if row[0]=='M' else 0, Vectors.dense(row[1])))    \\\n",
        "                    .map(lambda row: LabeledPoint(row[0], row[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iArvDprNCFEu",
        "outputId": "7e168b11-31e3-4120-fe82-cc2c33fc1ebd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[LabeledPoint(1.0, [17.99,10.38,122.8,1001.0,0.1184,0.2776,0.3001,0.1471,0.2419,0.07871,1.095,0.9053,8.589,153.4,0.006399,0.04904,0.05373,0.01587,0.03003,0.006193,25.38,17.33,184.6,2019.0,0.1622,0.6656,0.7119,0.2654,0.4601,0.1189]),\n",
              " LabeledPoint(1.0, [20.57,17.77,132.9,1326.0,0.08474,0.07864,0.0869,0.07017,0.1812,0.05667,0.5435,0.7339,3.398,74.08,0.005225,0.01308,0.0186,0.0134,0.01389,0.003532,24.99,23.41,158.8,1956.0,0.1238,0.1866,0.2416,0.186,0.275,0.08902])]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# look at some data\n",
        "lp.take(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tdCDCIJ0CFEv"
      },
      "outputs": [],
      "source": [
        "# Split data approximately into training (60%) and test (40%)\n",
        "training, test = lp.randomSplit([0.6, 0.4], seed=311)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3er6MC0vCFEw",
        "outputId": "a07d9a99-4998-41ef-a2cc-2dfe1bacd655"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(352, 217, 569)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# count records in datasets\n",
        "(training.count(), test.count(), lp.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqWOiQ54CFEx",
        "outputId": "62bfb5b6-1cd9-4619-8555-a40fe9f5b11d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6186291739894552, 0.38137082601054484, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "(training.count()/lp.count(), test.count()/lp.count(), lp.count()/lp.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "pHCJLn2FCFEx"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "model = LogisticRegressionWithLBFGS.train(training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnZNIVCTCFEy",
        "outputId": "f595a412-158d-431a-a864-b6738492118b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model accuracy (test): 0.9354838709677419\n"
          ]
        }
      ],
      "source": [
        "# Evaluating the model on test data\n",
        "labelsAndPreds_te = test.map(lambda p: (p.label, float(model.predict(p.features))))\n",
        "accuracy_te = 1.0 * labelsAndPreds_te.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n",
        "print('model accuracy (test): {}'.format(accuracy_te))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZB4EleOCFE0",
        "outputId": "9c7f53c6-f463-4a21-d67d-759448f1958e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1) accuracy: 0.93548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2) Confusion Matrix:\n",
            "[[123.   7.]\n",
            " [  7.  80.]]\n"
          ]
        }
      ],
      "source": [
        "from pyspark.mllib.evaluation import MulticlassMetrics # for confusion matrix\n",
        "\n",
        "print('(1) accuracy: {:.5f}'.format(accuracy_te))\n",
        "confmat = (MulticlassMetrics(labelsAndPreds_te)).confusionMatrix().toArray()\n",
        "print('(2) Confusion Matrix:')\n",
        "print(confmat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "fCLaGJ9lCFE5"
      },
      "outputs": [],
      "source": [
        "# Repeat model traning...\n",
        "assembler = VectorAssembler(inputCols=[\"f1\",\"f2\",\"f3\",\"f4\",\"f5\",\"f6\",\"f7\",\"f8\",\"f9\",\n",
        "                                        \"f10\",\"f11\",\"f12\",\"f13\",\"f14\",\"f15\",\"f16\",\"f17\",\n",
        "                                        \"f18\",\"f19\",\"f20\",\"f21\",\"f22\", \"f23\", \"f24\", \"f25\", \n",
        "                                       \"f26\", \"f27\", \"f28\", \"f29\", \"f30\"], \n",
        "                            outputCol=\"features\") \n",
        "\n",
        "# scaling\n",
        "transformed = assembler.transform(df)\n",
        "scaler = StandardScaler(inputCol='features',\n",
        "                       outputCol='scaledFeatures')\n",
        "sm = scaler.fit(transformed) # model\n",
        "sd = sm.transform(transformed) # data\n",
        "dataRdd = transformed.select(\"diagnosis\", \"features\").rdd.map(tuple)\n",
        "lp = dataRdd.map(lambda row:(1 if row[0]=='M' else 0, Vectors.dense(row[1])))    \\\n",
        "                    .map(lambda row: LabeledPoint(row[0], row[1]))\n",
        "training, test = lp.randomSplit([0.6, 0.4], seed=314)\n",
        "model = LogisticRegressionWithLBFGS.train(training, intercept=True)\n",
        "training.count(), test.count(), lp.count()\n",
        "\n",
        "labelsAndPreds_te = test.map(lambda p: (p.label, float(model.predict(p.features))))\n",
        "accuracy_te = 1.0 * labelsAndPreds_te.filter(lambda pl: pl[0] == pl[1]).count() / test.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPpS-vcDCFE6",
        "outputId": "c86f45bd-0508-448b-91bd-e6eec53eca35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1) accuracy: 0.95775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2) Confusion Matrix:\n",
            "[[134.   4.]\n",
            " [  5.  70.]]\n"
          ]
        }
      ],
      "source": [
        "print('(1) accuracy: {:.5f}'.format(accuracy_te))\n",
        "confmat = (MulticlassMetrics(labelsAndPreds_te)).confusionMatrix().toArray()\n",
        "print('(2) Confusion Matrix:')\n",
        "print(confmat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "JGxXh6j3CFE8"
      },
      "outputs": [],
      "source": [
        "# Repeat model traning...\n",
        "assembler = VectorAssembler(inputCols=[\"f1\",\"f2\",\"f3\",\"f4\",\"f5\",\"f6\",\"f7\",\"f8\",\"f9\",\n",
        "                                        \"f10\",\"f11\",\"f12\",\"f13\",\"f14\",\"f15\",\"f16\",\"f17\",\n",
        "                                        \"f18\",\"f19\",\"f20\",\"f21\",\"f22\", \"f23\", \"f24\", \"f25\", \n",
        "                                       \"f26\", \"f27\", \"f28\", \"f29\", \"f30\"], \n",
        "                            outputCol=\"features\") \n",
        "\n",
        "# scaling\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "transformed = assembler.transform(df)\n",
        "scaler = StandardScaler(inputCol='features',\n",
        "                       outputCol='scaledFeatures')\n",
        "sm = scaler.fit(transformed) # model\n",
        "sd = sm.transform(transformed) # data\n",
        "dataRdd = transformed.select(\"diagnosis\", \"features\").rdd.map(tuple)\n",
        "lp = dataRdd.map(lambda row:(1 if row[0]=='M' else 0, Vectors.dense(row[1])))    \\\n",
        "                    .map(lambda row: LabeledPoint(row[0], row[1]))\n",
        "####\n",
        "training, test = lp.randomSplit([0.7, 0.3], seed=314) #### RANDOM SPLIT 0.7, 0.3\n",
        "####\n",
        "model = LogisticRegressionWithLBFGS.train(training, intercept=False)\n",
        "labelsAndPreds_te = test.map(lambda p: (p.label, float(model.predict(p.features))))\n",
        "accuracy_te = 1.0 * labelsAndPreds_te.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vorGIwWsCFE9",
        "outputId": "271a7dc0-adc8-41d6-ba31-9d93fe1eaed3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1) accuracy: 0.94340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2) Confusion Matrix:\n",
            "[[98.  6.]\n",
            " [ 3. 52.]]\n"
          ]
        }
      ],
      "source": [
        "print('(1) accuracy: {:.5f}'.format(accuracy_te))\n",
        "confmat = (MulticlassMetrics(labelsAndPreds_te)).confusionMatrix().toArray()\n",
        "print('(2) Confusion Matrix:')\n",
        "print(confmat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff9LKMloCFE-",
        "outputId": "4e197e60-4d3e-4ced-8abf-fab1856ebcd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1) accuracy: 0.93711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2) Confusion Matrix:\n",
            "[[98.  7.]\n",
            " [ 3. 51.]]\n"
          ]
        }
      ],
      "source": [
        "# Repeat model traning...\n",
        "assembler = VectorAssembler(inputCols=[\"f1\",\"f2\",\"f3\",\"f4\",\"f5\",\"f6\",\"f7\",\"f8\",\"f9\",\n",
        "                                        \"f10\",\"f11\",\"f12\",\"f13\",\"f14\",\"f15\",\"f16\",\"f17\",\n",
        "                                        \"f18\",\"f19\",\"f20\",\"f21\",\"f22\", \"f23\", \"f24\", \"f25\", \n",
        "                                       \"f26\", \"f27\", \"f28\", \"f29\", \"f30\"], \n",
        "                            outputCol=\"features\") \n",
        "\n",
        "# scaling\n",
        "transformed = assembler.transform(df)\n",
        "scaler = StandardScaler(inputCol='features',\n",
        "                       outputCol='scaledFeatures')\n",
        "sm = scaler.fit(transformed) # model\n",
        "sd = sm.transform(transformed) # data\n",
        "dataRdd = transformed.select(\"diagnosis\", \"features\").rdd.map(tuple)\n",
        "lp = dataRdd.map(lambda row:(1 if row[0]=='M' else 0, Vectors.dense(row[1])))    \\\n",
        "                    .map(lambda row: LabeledPoint(row[0], row[1]))\n",
        "\n",
        "\n",
        "training, test = lp.randomSplit([0.7, 0.3], seed=314)\n",
        "model = LogisticRegressionWithLBFGS.train(training, intercept=True)\n",
        "training.count(), test.count(), lp.count()\n",
        "\n",
        "labelsAndPreds_te = test.map(lambda p: (p.label, float(model.predict(p.features))))\n",
        "accuracy_te = 1.0 * labelsAndPreds_te.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n",
        "\n",
        "print('(1) accuracy: {:.5f}'.format(accuracy_te))\n",
        "confmat = (MulticlassMetrics(labelsAndPreds_te)).confusionMatrix().toArray()\n",
        "print('(2) Confusion Matrix:')\n",
        "print(confmat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu25XeFuCFE_"
      },
      "source": [
        "The difference between part one and two was the addition of an intercept in the second model. Doing so ended up raising the accuracy of that model by a small amount (.96244 accuracy before vs .96714 after).\n",
        "\n",
        "The same difference was done between the third and fourth model, along with an adjustment of the training and test data inputs (60-40 to 70-30). This time, adding an intercept lowered the accuracy of the model (.94340 accuracy before vs .93711 accuracy after). However, even with this difference, both of my first two models had higher accuracies than either of the latter two, suggesting that using an intercept and training data split at 60-40 leads to the most accurate model for this data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2YJiHubCFE_",
        "outputId": "d8755e26-78da-415e-8658-cf3c2c01240b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NbConvertApp] Converting notebook /sfs/qumulo/qhome/dbw2tn/ds5110/assignments/M4_8_classification/classification_wisc_breast_cancer.ipynb to pdf\n",
            "[NbConvertApp] Writing 61214 bytes to notebook.tex\n",
            "[NbConvertApp] Building PDF\n",
            "[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n",
            "[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n",
            "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
            "[NbConvertApp] PDF successfully created\n",
            "[NbConvertApp] Writing 70211 bytes to /sfs/qumulo/qhome/dbw2tn/ds5110/assignments/M4_8_classification/classification_wisc_breast_cancer.pdf\n"
          ]
        }
      ],
      "source": [
        "!jupyter nbconvert --to pdf `pwd`/*.ipynb"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "DS 5110",
      "language": "python",
      "name": "ds5110"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "classification_cancer_algo.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}